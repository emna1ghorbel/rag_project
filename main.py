import os
import sys
from loader import load_full_cti_dataset
from processor import get_text_chunks
from vectorstore import create_or_update_vectorstore, load_local_vectorstore
from rag_engine import get_rag_chain

# Configuration pour l'affichage Windows (UTF-8)
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

def main():
    # --- CONFIGURATION DES CHEMINS ---
    POSTS_PATH = "data/posts" 
    REPLIES_PATH = "data/replies"
    INDEX_NAME = "faiss_index_cti"

    print("=== SYST√àME CTI DARKGRAM (PHI-3.5 + OLLAMA) ===")

    # --- √âTAPE 1, 2 & 3 : PR√âPARATION DES DONN√âES ---
    # On v√©rifie si l'index FAISS existe d√©j√† pour gagner du temps
    if os.path.exists(INDEX_NAME):
        print(f"üìÅ Index '{INDEX_NAME}' d√©tect√©. Chargement en cours...")
        vector_db = load_local_vectorstore(INDEX_NAME)
    else:
        print("üöÄ Index non trouv√©. Initialisation du pipeline complet...")
        
        # 1. Chargement et Nettoyage (Page 1)
        raw_documents = load_full_cti_dataset(POSTS_PATH, REPLIES_PATH)
        if not raw_documents:
            print("‚ùå Erreur : Aucun document n'a √©t√© charg√©. V√©rifiez vos dossiers data/.")
            return

        # 2. D√©coupage en Chunks (Page 2)
        print(f"‚úÇÔ∏è D√©coupage de {len(raw_documents)} documents...")
        final_chunks = get_text_chunks(raw_documents)

        # 3. Cr√©ation de la base vectorielle (Page 4)
        print("üèóÔ∏è Cr√©ation de la base FAISS (cela peut prendre du temps selon votre CPU)...")
        vector_db = create_or_update_vectorstore(final_chunks, INDEX_NAME)

    # --- √âTAPE 4 : INITIALISATION DU MOTEUR RAG (Page 5) ---
    print("ü§ñ Connexion √† Phi-3.5 via Ollama...")
    try:
        rag_system = get_rag_chain(vector_db)
        print("‚úÖ Syst√®me pr√™t pour l'analyse !\n")
    except Exception as e:
        print(f"‚ùå Erreur de connexion √† Ollama : {e}")
        print("Assurez-vous qu'Ollama est lanc√© et que le mod√®le phi3.5 est t√©l√©charg√©.")
        return

    # --- √âTAPE 5 : BOUCLE DE CHAT (INTERFACE TERMINAL) ---
    print("--- POSEZ VOS QUESTIONS (tapez 'exit' pour quitter) ---")
    while True:
        query = input("\nüîç Question CTI : ")
        
        if query.lower() in ['exit', 'quitter', 'quit']:
            print("üëã Fermeture du syst√®me.")
            break

        if not query.strip():
            continue

        print("‚è≥ Recherche et analyse en cours...")
        try:
            # Appel de la cha√Æne RAG
            response = rag_system.invoke({"query": query})
            
            # Affichage de la r√©ponse de Phi-3.5
            print("\nüìù ANALYSE DE L'IA :")
            print(response["result"])
            
            # Affichage des sources (Optionnel mais recommand√© pour le PFE)
            print("\nüìö SOURCES R√âCUP√âR√âES :")
            sources = set()
            for doc in response["source_documents"]:
                # On r√©cup√®re le nom du fichier ou de la source dans les metadata
                source_info = doc.metadata.get('source', 'Inconnu')
                sources.add(source_info)
            
            for s in sources:
                print(f"- {s}")
                
        except Exception as e:
            print(f"‚ùå Une erreur est survenue lors de la g√©n√©ration : {e}")

if __name__ == "__main__":
    main()